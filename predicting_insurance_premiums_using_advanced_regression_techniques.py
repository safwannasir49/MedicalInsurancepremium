# -*- coding: utf-8 -*-
"""Predicting Insurance Premiums Using Advanced Regression Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gryjdQbxc1LCBF2FsAAFpsi7kZKWI-an

# 1. Data Loading
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
import plotly.express as px
import plotly.graph_objects as go
import joblib
from flask import Flask, request, jsonify

# Set up Kaggle API credentials (ensure you have kaggle.json in ~/.kaggle or configure it here)
os.environ['KAGGLE_CONFIG_DIR'] = "~/.kaggle"

# Download the dataset from Kaggle
!kaggle datasets download -d moneystore/agencyperformance
!unzip agencyperformance.zip

# Load the dataset
data = pd.read_csv('finalapi.csv')

"""# 2. Data Cleaning and Preprocessing




"""

print("The numer of rows are: ",data.shape[0], "\nThe number of columns are: ",data.shape[1])

print("\n",data.columns)

data.columns

"""**Assigning numeric labels to Categories**



Few of the important variables that are requried for the anlaysis further are mapped with numerical values based on their categories. This is done for further feasibility of analysis and building of the models.
"""

categorical_columns = ['PROD_ABBR', 'PROD_LINE', 'STATE_ABBR', 'VENDOR']

# Map each categorical column to numeric values
for col in categorical_columns:
    unique_values = data[col].unique()
    mapping = {value: idx for idx, value in enumerate(unique_values)}
    data[col] = data[col].map(mapping)
    print(f"\nThe Unique values in the {col} Variable are: \n{data[col].unique()}")


print("\nUpdated Data Types:\n", data.dtypes)

"""**Checking for Class Imbalance**"""

target_column = 'STATE_ABBR'


print("The counts of observations in each class are: ")
print(data[target_column].value_counts())

print("\nThe percentage of classes are:")
class_percentages = (data[target_column].value_counts() / float(len(data))) * 100
print(class_percentages)


plt.figure(figsize=(12, 6))
class_percentages.plot.bar()
plt.title('Percentage Distribution of Each Class in {}'.format(target_column))
plt.xlabel('Class')
plt.ylabel('Percentage')
plt.xticks(rotation=90)
plt.show()

"""**Check for null values**"""

print(data.head())

"""**Removing null values**"""

data.dropna(inplace=True)

"""**Check for duplicates**"""

print(data.duplicated().sum())

"""**Distribution of Numerical Variables**"""

data.hist(bins=30, figsize=(50, 45))
plt.show()

"""**Encode categorical variables using Label Encoding**"""

label_encoders = {}
categorical_columns = data.select_dtypes(include=['object']).columns

for col in categorical_columns:
    label_encoders[col] = LabelEncoder()
    data[col] = label_encoders[col].fit_transform(data[col])

"""**Summary Of Data**"""

print(data.describe())

"""# 3. Exploratory Data Analysis (EDA)

**Distribution of Numerical Variables**
"""

data.hist(bins=30, figsize=(50, 45))
plt.show()

"""**Box Plots for the Numerical Variables to Check for Outliers**"""

plt.figure(figsize=(20, 15))
sns.boxplot(data=data)
plt.title('Box Plots of Numerical Variables')
plt.xticks(rotation=90)
plt.show()

"""**Correlation Analysis**"""

plt.figure(figsize=(50, 45))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix \n', fontsize=50)
plt.show()

"""**Summary statistics**"""

summary = data.describe()
print(summary)

"""# 4. Feature Engineering

**Scaling the Data**
"""

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data.drop('NB_WRTN_PREM_AMT', axis=1))
X = pd.DataFrame(scaled_data, columns=data.columns.drop('NB_WRTN_PREM_AMT'))
y = data['NB_WRTN_PREM_AMT']

y.unique()

"""**Splitting the Data into Train and Testing Sets**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X.head()

"""**Training and Testing Set for X**"""

print("{0:0.2f}% data is in training set".format((len(X_train)/len(data.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(X_test)/len(data.index)) * 100))

"""**Training and Testing Set for y**"""

print("{0:0.2f}% data is in training set".format((len(y_train)/len(data.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(y_test)/len(data.index)) * 100))

"""# 5. Model Building and Evaluation

**Building the Multiple Linear Regression Model**
"""

model = LinearRegression()
model.fit(X_train, y_train)

"""**Making Predictions**"""

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

"""**Evaluating the Model**"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

def evaluate_model(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

train_metrics = evaluate_model(y_train, y_train_pred)
test_metrics = evaluate_model(y_test, y_test_pred)

print("Train Metrics: MAE: {:.4f}, MSE: {:.4f}, RMSE: {:.4f}, R2: {:.4f}".format(*train_metrics))
print("Test Metrics: MAE: {:.4f}, MSE: {:.4f}, RMSE: {:.4f}, R2: {:.4f}".format(*test_metrics))

"""**Plotting the Model**"""

plt.figure(figsize=(12, 6))
plt.scatter(y_test, y_test_pred, alpha=0.3)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Values (Test Set)')
plt.show()

"""# 6. Cross Validation"""

from sklearn.model_selection import cross_val_score

# Cross-Validation
model = LinearRegression()
cv_scores = cross_val_score(model, X, y, cv=10, scoring='r2')
print(f'Cross-Validation R2 Scores: {cv_scores}')
print(f'Mean Cross-Validation R2 Score: {cv_scores.mean()}')

plt.figure(figsize=(10, 6))
sns.lineplot(cv_scores)
plt.title('Cross-Validation R2 Scores')
plt.xlabel('R2 Score')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(cv_scores)
plt.title('Cross-Validation R2 Scores')
plt.xlabel('R2 Score')
plt.show()

"""**Alternatively, using a bar plot**"""

plt.figure(figsize=(10, 6))
sns.barplot(x=list(range(1, 11)), y=cv_scores)
plt.title('Cross-Validation R2 Scores')
plt.xlabel('Fold')
plt.ylabel('R2 Score')
plt.show()

"""# 7. Advanced Feature Engineering and Ensemble Methods

**Ridge Regression**
"""

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV


ridge = Ridge()
ridge_params = {'alpha': [0.1, 1.0, 10.0]}
ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='r2')
ridge_grid.fit(X_train, y_train)
print(f'Best Ridge Parameters: {ridge_grid.best_params_}')

"""**Lasso Regression**"""

lasso = Lasso()
lasso_params = {'alpha': [0.1, 1.0, 10.0]}
lasso_grid = GridSearchCV(lasso, lasso_params, cv=5, scoring='r2')
lasso_grid.fit(X_train, y_train)
print(f'Best Lasso Parameters: {lasso_grid.best_params_}')

"""**ElasticNet Regression**"""

elasticnet = ElasticNet()
elasticnet_params = {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.2, 0.5, 0.8]}
elasticnet_grid = GridSearchCV(elasticnet, elasticnet_params, cv=5, scoring='r2')
elasticnet_grid.fit(X_train, y_train)
print(f'Best ElasticNet Parameters: {elasticnet_grid.best_params_}')

"""# 8. Visualizations

**Retention Policies VS Written Premium**
"""

fig = px.scatter(data, x='RETENTION_POLY_QTY', y='NB_WRTN_PREM_AMT', color='STATE_ABBR', title='Retention Policies vs Written Premiums (Scatter Plot)')
fig.show()

"""**State-wise Distribution of Written Premiums**"""

fig = px.box(data, x='STATE_ABBR', y='NB_WRTN_PREM_AMT', title='State-wise Distribution of Written Premiums')
fig.show()

"""**HeatMap of Missing Values**"""

plt.figure(figsize=(10, 8))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Heatmap of Missing Values')
plt.show()

"""**Distribution of Retention Ratio by State**"""

plt.figure(figsize=(15, 8))
statewise_retention = data.groupby('STATE_ABBR')['RETENTION_RATIO'].mean().sort_values(ascending=False)
statewise_retention.plot(kind='bar', color='lightgreen')
plt.title('State-wise Distribution of Retention Ratios')
plt.xlabel('State')
plt.ylabel('Average Retention Ratio')
plt.xticks(rotation=45)
plt.show()

"""**Growth Rate over the Years**"""

plt.figure(figsize=(12, 6))
data.groupby('STAT_PROFILE_DATE_YEAR')['GROWTH_RATE_3YR'].mean().plot(kind='line', marker='o', linestyle='-', color='red')
plt.title('Growth Rate over the Years')
plt.xlabel('Year')
plt.ylabel('Average Growth Rate (3 Years)')
plt.xticks(data['STAT_PROFILE_DATE_YEAR'].unique())
plt.show()

"""**Loss Ratio vs. Written Premium**"""

plt.figure(figsize=(12, 6))
sns.scatterplot(x='LOSS_RATIO', y='WRTN_PREM_AMT', data=data, alpha=0.7, hue='STATE_ABBR', palette='viridis')
plt.title('Loss Ratio vs. Written Premium')
plt.xlabel('Loss Ratio')
plt.ylabel('Written Premium')
plt.legend(loc='upper right')
plt.show()

"""**Average Premium by Product Line**"""

plt.figure(figsize=(12, 6))
product_line_premiums = data.groupby('PROD_LINE')['WRTN_PREM_AMT'].mean().sort_values(ascending=False)
product_line_premiums.plot(kind='bar', color='coral')
plt.title('Average Premium by Product Line')
plt.xlabel('Product Line')
plt.ylabel('Average Written Premium')
plt.xticks(rotation=45)
plt.show()

"""**Retention Policies vs Written Premiums**"""

fig = px.scatter(data, x='RETENTION_POLY_QTY', y='NB_WRTN_PREM_AMT', color='STATE_ABBR', title='Retention Policies vs Written Premiums (Scatter Plot)')
fig.show()

"""# Deployment

**1. Save Your Model**

First, ensure your model is saved in a format that can be easily loaded for inference. For example, if you're using scikit-learn, you can use joblib to save the model.
"""

import joblib


joblib.dump(model, 'model.pkl')

"""**2. Set Up a Flask Application**


Flask is a lightweight web framework for Python. You'll create a Flask application that exposes an API endpoint to interact with your model.

Create a Flask App
Create a file named app.py for your Flask application:
"""

from flask import Flask, request, jsonify
import joblib
import numpy as np

# Load the model
model = joblib.load('model.pkl')

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():

    data = request.get_json(force=True)


    features = np.array(data['features']).reshape(1, -1)


    prediction = model.predict(features)


    return jsonify({'prediction': prediction[0]})

if __name__ == '__main__':
    app.run(debug=True)

Explanation:
Import Libraries: Import Flask and necessary libraries.
Load Model: Load the pre-trained model using joblib.
Create Flask App: Initialize the Flask app.
Define Endpoint: Define a POST endpoint /predict that accepts JSON data with features, makes a prediction using the model, and returns the result.

"""**3. Run the Flask Application**


To start the Flask server, run the following command in your terminal:
"""

python app.py

"""**4. Test the Deployment**

You can test your deployed model using tools like curl, Postman, or directly from Python.
"""

curl -X POST -H "Content-Type: application/json" -d '{"features": [value1, value2, ...]}' http://127.0.0.1:5000/predict

"""Replace [value1, value2, ...] with the actual feature values.

**5. Deploy to a Cloud Service**

For production deployment, consider using cloud services like AWS, Google Cloud, or Azure:

AWS Elastic Beanstalk: Deploy your Flask app to AWS Elastic Beanstalk.
Google Cloud App Engine: Deploy your Flask app to Google Cloud App Engine.
Heroku: Deploy your Flask app to Heroku with a Procfile specifying the command to run the Flask app.

Example Procfile for Heroku:
Create a Procfile in your project directory with the following content
"""

web: python app.py

"""Conclusion
Deploying a machine learning model involves saving the model, setting up a web service, and integrating it with production systems. Flask provides a simple and effective way to deploy your model, but for production-ready deployments, consider cloud platforms that offer scalability, security, and monitoring features.
"""